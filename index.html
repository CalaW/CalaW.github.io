<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Chen  Chen</title>
    <meta name="author" content="Chen  Chen">
    <meta name="description" content="Chen Chen's personal webpage. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="robotics, computer science, human robot interaction">

    
    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">
    
    <!-- Fonts & Icons -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;family=Noto+Serif+SC&amp;display=swap" rel="stylesheet">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%BE&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://calaw.cc/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark">
    
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    
    
  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/awards/">Awards</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Chen</span>  Chen - 陈辰
          </h1>
          <p class="desc"><em>Dare mighty things.</em></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/my_photo-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/my_photo-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/my_photo-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/my_photo.jpeg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="my_photo.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I am a third-year undergraduate student at Tsinghua University (THU), majoring in automation.</p>

<p>My research interests include <strong><em>human robot interaction</em></strong> and <strong><em>medical and healthcare robots</em></strong>.</p>

<p>I have been working as a research assistant at <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Intelligent Robotic Manipulation Lab</a> in Tsinghua since 2021, advised by Professor <a href="https://scholar.google.com.sg/citations?hl=zh-CN&amp;user=6EIX-JQAAAAJ" rel="external nofollow noopener" target="_blank">Xiang Li</a>.
Prior to that, I was a research intern at <a href="https://github.com/AIR-DISCOVER" rel="external nofollow noopener" target="_blank">DISCOVER Lab</a>.</p>

<p>I will start working as a research intern at <a href="https://rchi-lab.github.io" rel="external nofollow noopener" target="_blank">Robotic Caregiving and Human Interaction (RCHI) lab</a> in Carnegie Mellon University this summer, advised by Professor <a href="https://zackory.com" rel="external nofollow noopener" target="_blank">Zackory Erickson</a>.</p>

          </div>

          <!-- News -->
          

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">Publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/icra2022-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/icra2022-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/icra2022-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/icra2022.jpeg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="icra2022.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="yanicra2022" class="col-sm-8">
        <!-- Title -->
        <div class="title">Adaptive Vision-Based Control of Redundant Robots with Null-Space Interaction for Human-Robot Collaboration</div>
        <!-- Author -->
        <div class="author">
        

        Xiangjie Yan, <em>Chen Chen</em>, and Xiang Li</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 2022 International Conference on Robotics and Automation (ICRA)</em>,
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://github.com/yanseim/Vision-Based-Control" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Human-robot collaboration aims to extend human ability through cooperation with robots. This technology is currently helping people with physical disabilities, has transformed the manufacturing process of companies, improved surgical performance, and will likely revolutionize the daily lives of everyone in the future. Being able to enhance the performance of both sides, such that human-robot collaboration outperforms a single robot/human, remains an open issue. For safer and more effective collaboration, a new control scheme has been proposed for redundant robots in this paper, consisting of an adaptive vision-based control term in task space and an interactive control term in null space. Such a formulation allows the robot to autonomously carry out tasks in an unknown environment without prior calibration while also interacting with humans to deal with unforeseen changes (e.g., potential collision, temporary needs) under the redundant configuration. The decoupling between task space and null space helps to explore the collaboration safely and effectively without affecting the main task of the robot end-effector. The stability of the closed-loop system has been rigorously proved with Lyapunov methods, and both the convergence of the position error in task space and that of the damping model in null space are guaranteed. The experimental results of a robot manipulator guided with the technology of augmented reality (AR) are presented to illustrate the performance of the control scheme.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yanicra2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adaptive {{Vision-Based Control}} of {{Redundant Robots}} with {{Null-Space Interaction}} for {{Human-Robot Collaboration}}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Xiangjie and Chen, Chen and Li, Xiang}</span><span class="p">,</span>
  <span class="na">date</span> <span class="p">=</span> <span class="s">{2022-05}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2803--2809}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA46639.2022.9812218}</span><span class="p">,</span>
  <span class="na">eventtitle</span> <span class="p">=</span> <span class="s">{2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Adaptation models,Aerospace electronics,Collaboration,Damping,End effectors,Null space,Stability analysis}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/tcst2023-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/tcst2023-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/tcst2023-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/tcst2023.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="tcst2023.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="yan2022" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Complementary Framework for Human-Robot Collaboration with a Mixed AR-Haptic Interface</div>
        <!-- Author -->
        <div class="author">
        

        Xiangjie Yan, Yongpeng Jiang, <em>Chen Chen</em>, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Leiliang Gong, Ming Ge, Tao Zhang, Xiang Li' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2210.06003" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>There is invariably a trade-off between safety and efficiency for collaborative robots (cobots) in human-robot collaborations. Robots that interact minimally with humans can work with high speed and accuracy but cannot adapt to new tasks or respond to unforeseen changes, whereas robots that work closely with humans can but only by becoming passive to humans, meaning that their main tasks suspended and efficiency compromised. Accordingly, this paper proposes a new complementary framework for human-robot collaboration that balances the safety of humans and the efficiency of robots. In this framework, the robot carries out given tasks using a vision-based adaptive controller, and the human expert collaborates with the robot in the null space. Such a decoupling drives the robot to deal with existing issues in task space (e.g., uncalibrated camera, limited field of view) and in null space (e.g., joint limits) by itself while allowing the expert to adjust the configuration of the robot body to respond to unforeseen changes (e.g., sudden invasion, change of environment) without affecting the robot’s main task. Additionally, the robot can simultaneously learn the expert’s demonstration in task space and null space beforehand with dynamic movement primitives (DMP). Therefore, an expert’s knowledge and a robot’s capability are both explored and complementary. Human demonstration and involvement are enabled via a mixed interaction interface, i.e., augmented reality (AR) and haptic devices. The stability of the closed-loop system is rigorously proved with Lyapunov methods. Experimental results in various scenarios are presented to illustrate the performance of the proposed method.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">yan2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A {{Complementary Framework}} for {{Human-Robot Collaboration}} with a {{Mixed AR-Haptic Interface}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Xiangjie and Jiang, Yongpeng and Chen, Chen and Gong, Leiliang and Ge, Ming and Zhang, Tao and Li, Xiang}</span><span class="p">,</span>
  <span class="na">date</span> <span class="p">=</span> <span class="s">{2022-10-12}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{arXiv:2210.06003}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{arXiv:2210.06003}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2210.06003}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2023-02-12}</span><span class="p">,</span>
  <span class="na">pubstate</span> <span class="p">=</span> <span class="s">{preprint}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Robotics}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/iros2023-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/iros2023-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/iros2023-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/iros2023.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="iros2023.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>

        <!-- Entry bib key -->
        <div id="yan2023" class="col-sm-8">
        <!-- Title -->
        <div class="title">Multi-Modal Interaction Control of Ultrasound Scanning Robots with Safe Human Guidance and Contact Recovery</div>
        <!-- Author -->
        <div class="author">
        

        Xiangjie Yan, Yongpeng Jiang, Guokun Wu, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Chen Chen, Gao Huang, Xiang Li' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2302.05685" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Ultrasound scanning robots enable the automatic imaging of a patient’s internal organs by maintaining close contact between the ultrasound probe and the patient’s body during a scanning procedure. Comprehensive, high-quality ultrasound scans are essential for providing the patient with an accurate diagnosis and effective treatment plan. An ultrasound scanning robot usually works in a doctor-robot co-existing environment, hence both efficiency and safety during the collaboration should be considered. In this paper, we propose a novel multi-modal control scheme for ultrasound scanning robots, in which three interaction modes are integrated into a single control input. Specifically, the scanning mode drives the robot to track a time-varying trajectory on the patient’s body under the desired impedance model; the recovery mode allows the robot to actively recontact the body whenever physical contact between the ultrasound probe and the patient’s body is lost; the human-guided mode renders the robot passive such that the doctor can safely intervene to manually reposition the probe. The integration of multiple modes allows the doctor to intervene safely at any time during the task and also maximizes the robot’s autonomous scanning ability. The performance of the robot is validated on a collaborative scanning task of a carotid artery examination.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">yan2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-{{Modal Interaction Control}} of {{Ultrasound Scanning Robots}} with {{Safe Human Guidance}} and {{Contact Recovery}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Xiangjie and Jiang, Yongpeng and Wu, Guokun and Chen, Chen and Huang, Gao and Li, Xiang}</span><span class="p">,</span>
  <span class="na">date</span> <span class="p">=</span> <span class="s">{2023-02-11}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{arXiv:2302.05685}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{arXiv:2302.05685}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2302.05685}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2023-03-16}</span><span class="p">,</span>
  <span class="na">pubstate</span> <span class="p">=</span> <span class="s">{preprint}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Robotics}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


          <!-- Social -->
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%63%68%65%6E-%63%68%65%32%30@%6D%61%69%6C%73.%74%73%69%6E%67%68%75%61.%65%64%75.%63%6E" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0003-2588-7376" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=WV7thfcAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/CalaW" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            

              </div>

              <div class="contact-note">
                Best way to reach me is via email.

              </div>

            </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Chen  Chen. Last updated: April 25, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-79BFQVQYLC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-79BFQVQYLC');
  </script>
    

  </body>
</html>
